{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e7eaa1",
   "metadata": {},
   "source": [
    "This notebook carries functions necessary to perform CDkM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922512bb",
   "metadata": {},
   "source": [
    "## packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6970f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "# from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.cluster import KMeans\n",
    "## plotting packages\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.validators.scatter.marker import SymbolValidator\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "## other helpful packages \n",
    "from itertools import combinations, permutations, combinations_with_replacement\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "# !pip install python-louvain \n",
    "import community\n",
    "\n",
    "## import these dictionaries (find on Github (Translation/Helper), store in directory)\n",
    "from pid2pos_bref2nba_nba2bref_pid2name_name2pid import *\n",
    "\n",
    "import decimal\n",
    "\n",
    "def round_down(value, decimals):\n",
    "    with decimal.localcontext() as ctx:\n",
    "        d = decimal.Decimal(value)\n",
    "        ctx.rounding = decimal.ROUND_DOWN\n",
    "        return round(d, decimals)\n",
    "    \n",
    "import pickle\n",
    "import math\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d96d2d",
   "metadata": {},
   "source": [
    "### read in dataframes\n",
    "\n",
    "User will need to create their own directory / folder containing dfs (stored as csv on Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDFs = [\n",
    "   'MasterClutch','MasterDefense', 'MasterRebound', 'MasterPassing', 'MasterScoring' ,'MasterMisc',  \n",
    "]\n",
    "\n",
    "folder = 'folder_name'\n",
    "\n",
    "for dfStr in allDFs:\n",
    "    vars()[dfStr] = pd.read_csv(f'{folder}/{dfStr}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7cae9",
   "metadata": {},
   "source": [
    "### begin CDkM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c116137",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper ###\n",
    "# drop unneeded/redundant columns\n",
    "colDrop = [\n",
    "    'Unnamed: 0', 'GP', 'Season', 'comb', 'PLAYER_ID'\n",
    "]\n",
    "\n",
    "def col2str(item):\n",
    "        return str(item)\n",
    "    \n",
    "### pre-processing ###\n",
    "def scale_and_count_DFs(DFs, seasons):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    DFs = allDFs\n",
    "    season = specified by user\n",
    "    \n",
    "    OUTPUT\n",
    "    Counter(totAppear): dictionary; counts number of times players appear in same DF; used as denominator when building network arc weights. \n",
    "    idx_DFs: placeholder DF; contains general items of interest for tracking and reporting\n",
    "    scaled_DFs: returns Standard Scaler'ed version of original DFs for PCA and other analysis\n",
    "    \"\"\"\n",
    "    # standardize data\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    idx_DFs = []\n",
    "    scaled_DFs = []\n",
    "    totAppear = []\n",
    "    for dfStr in DFs:\n",
    "        vars()[dfStr] = pd.read_csv(f'{folder}/{dfStr}.csv')\n",
    "        temp = vars()[dfStr].copy()\n",
    "        allYears = []\n",
    "        for s in seasons: \n",
    "            temp0 = temp[(temp.Season == s)].copy()\n",
    "            allYears.append(temp0)\n",
    "        temp = pd.concat(allYears, ignore_index=True)      \n",
    "\n",
    "        # create idx tracker\n",
    "        vars()[dfStr + '_idx'] = temp.loc[:,['GP', 'Season', 'comb', 'PLAYER_ID']]\n",
    "        idx_DFs.append(vars()[dfStr + '_idx'])\n",
    "\n",
    "        # count how many times players appear to scale counts later\n",
    "        temp_ = temp.sort_values(by='PLAYER_ID')\n",
    "        temp_ = temp_['PLAYER_ID'].apply(col2str) + '_' + temp_['Season'].apply(col2str)\n",
    "        comb = list(combinations(temp_, 2))\n",
    "        totAppear.extend(comb)\n",
    "\n",
    "\n",
    "        # subset into kept columns\n",
    "        temp = temp.drop(columns=colDrop)\n",
    "\n",
    "        # create scaled df\n",
    "        vars()[dfStr + '_scaled'] = scaler.fit_transform(temp)\n",
    "        scaled_DFs.append(vars()[dfStr + '_scaled'])\n",
    "    return Counter(totAppear), idx_DFs, scaled_DFs\n",
    "\n",
    "def pca_tf(DFs, scaled_DFs, var2keep): \n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    var2keep: scaler; variance to keep for PCA\n",
    "    \n",
    "    OUTPUT\n",
    "    allPCA: PCA transformed DFs\n",
    "    \"\"\"\n",
    "    ## pca first\n",
    "    allPCA = []\n",
    "    comp2keep = {}\n",
    "    running = 0\n",
    "    for idx, dfStr in enumerate(DFs):\n",
    "        # get number of components to keep\n",
    "        temp = scaled_DFs[idx]\n",
    "        pca = PCA().fit(temp)\n",
    "        comp2keep[dfStr + '_scaled'] = np.where(np.cumsum(pca.explained_variance_ratio_)>var2keep)[0][0]\n",
    "        running += np.where(np.cumsum(pca.explained_variance_ratio_)>var2keep)[0][0]  \n",
    "        \n",
    "    for k,v in comp2keep.items(): \n",
    "        print('For {}, {} components were kept.'.format(k,v))\n",
    "    \n",
    "    # then transofrm scaled dfs into pca dfs\n",
    "    for idx, dfStr in enumerate(DFs):\n",
    "        newStr = dfStr + '_scaled'\n",
    "        temp = scaled_DFs[idx]\n",
    "        best_k = comp2keep[newStr]\n",
    "        pca_tf = PCA(n_components=best_k)\n",
    "        dfPca = pca_tf.fit_transform(temp)\n",
    "        allPCA.append(dfPca)\n",
    "    return allPCA\n",
    "\n",
    "### CDkM algorithm ###\n",
    "## now begin clustering for each df, keep track of connections in a dictionary\n",
    "\n",
    "# create results dataframe after clustering using kMeans\n",
    "def clusterResults(idx_DFs, scaled_DFs, n_list, pid2pos, clusterMethod='kmeans'):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    n_list: range of k's to use; ex: [[i]*numdf for i in range(2,151)]\n",
    "    \n",
    "    OUTPUT\n",
    "    allResults: list of resulting cluster DFs (one for each master DF); cols = ['GP', 'szn', 'comb', 'pid', 'cluster', 'df', 'pos']\n",
    "    \"\"\"\n",
    "    allResults = []\n",
    "    for i in range(len(idx_DFs)):\n",
    "        df = scaled_DFs[i]\n",
    "        kmeans = KMeans(n_clusters=n_list[i], random_state=13).fit(df)\n",
    "        labels_ = kmeans.labels_\n",
    "        results = pd.DataFrame(columns=['GP', 'szn', 'comb', 'pid', 'cluster', 'pos', 'df'])\n",
    "        results[['GP', 'szn', 'comb', 'pid']] = idx_DFs[i]\n",
    "        results['cluster'] = labels_\n",
    "        results['df'] = allDFs[i]\n",
    "        for idx, row in results.iterrows():\n",
    "            pl = str(row.pid) + '_' + str(row.szn)\n",
    "            if pl in pid2pos:\n",
    "                results.loc[idx,'pos'] = pid2pos[pl]\n",
    "            else:\n",
    "                results.loc[idx,'pos'] = 'nan'\n",
    "        allResults.append(results)\n",
    "    return allResults\n",
    "\n",
    "# count how often each pair occurs in same cluster\n",
    "def createCountDict(allResults, pid2name):\n",
    "    \"\"\"\n",
    "    OUTPUT\n",
    "    countDict: key: player pair; value: raw # times pair appear in same cluster\n",
    "    countDictMatch: key: player pair; value: which master DF categories they match in for post analysis\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    countDictMatch = {}\n",
    "    for ix,df in enumerate(allResults):\n",
    "        df = df.sort_values(by='pid')\n",
    "        for cl in df.cluster.unique():\n",
    "            temp = df[(df.cluster==cl)]\n",
    "            nodes = temp['pid'].apply(col2str) + '_' + temp['szn'].apply(col2str)\n",
    "            comb = list(combinations(nodes,2))\n",
    "            for pair in comb:\n",
    "                if int(pair[0].split('_')[0]) in pid2name and int(pair[1].split('_')[0]) in pid2name:\n",
    "                    if pid2name[int(pair[0].split('_')[0])] == pid2name[int(pair[1].split('_')[0])]:\n",
    "                        continue\n",
    "                    if pair in countDict:\n",
    "                        countDict[pair] += 1\n",
    "                        countDictMatch[pair].append(allDFs[ix])\n",
    "                    else:\n",
    "                        countDict[pair] = 1  \n",
    "                        countDictMatch[pair] = [allDFs[ix]]\n",
    "    return countDict, countDictMatch\n",
    "\n",
    "# different ways for determining arc weights (for testing, user can try out different schemes)\n",
    "def calcVals(countDict, numAppear, destStr):\n",
    "    \"\"\"\n",
    "    OUTPUT\n",
    "    weightDF: write scaled arc-weight results to csv\n",
    "    \"\"\"\n",
    "    with open(destStr, 'w') as f:\n",
    "        f.write('player1,player2,val_3,val_6,raw,total\\n')\n",
    "        for key in countDict.keys():\n",
    "            if numAppear[key] == 0:\n",
    "                continue\n",
    "            \n",
    "            # zero or one\n",
    "            v1 = int(countDict[key]/numAppear[key])\n",
    "            \n",
    "            # round to two decimals\n",
    "            v2 = float(round_down(countDict[key]/numAppear[key], 2))\n",
    "            \n",
    "            v7 = countDict[key]\n",
    "            \n",
    "            if v2 <= 0.2:\n",
    "                v3 = 0\n",
    "            elif v2 <= 0.4:#0.35:\n",
    "                v3 = 1\n",
    "            elif v2 <= 0.6:#0.5:\n",
    "                v3 = 2\n",
    "            elif v2 <= 0.8:#0.7:\n",
    "                v3 = 3\n",
    "#             elif v2 <= 0.85:\n",
    "#                 v3 = 4\n",
    "            else:\n",
    "                v3 = 4\n",
    "                \n",
    "            v4 = float(round_down(countDict[key]/numAppear[key], 2))\n",
    "\n",
    "            \n",
    "            if v4 <= 0.25:\n",
    "                v6 = 0\n",
    "            elif v4 <= 0.5:\n",
    "                v6 = 1\n",
    "            elif v4 <= 0.75:\n",
    "                v6 = 2\n",
    "            else:\n",
    "                v6 = 3\n",
    "                        \n",
    "            f.write('{},{},{},{},{},{}\\n'.format(key[0],key[1],v3,v6,v4,v7))\n",
    "        f.close()\n",
    "        return pd.read_csv(destStr)  \n",
    "\n",
    "# using weights found, perform community detection\n",
    "def getModularity(nodeDF, valDF):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    nodeDF: from weightDF, players (2 columns)\n",
    "    valDF: from weightDF, value chosen (1 column)\n",
    "    \n",
    "    OUTPUT\n",
    "    partition: output from Louvain algorithm\n",
    "    G: create graph using networkx\n",
    "    mod: scalar; modularity of Louvain algorithm results\n",
    "    numGrps: scalar; |partitions|\n",
    "    \"\"\"\n",
    "    # create df\n",
    "    graphDF = pd.concat([nodeDF, valDF], axis=1)\n",
    "    graphDF.columns = ['node1', 'node2', 'value']\n",
    "    # build graph\n",
    "    G = nx.Graph()\n",
    "    for idx,row in graphDF.iterrows():\n",
    "        G.add_edge(row.node1, row.node2, weight=row.value)\n",
    "    # partition into clusters\n",
    "    partition = community.best_partition(G,randomize=False, random_state=13)\n",
    "    uniqueGrp = set()\n",
    "    for key,val in partition.items():\n",
    "        uniqueGrp.add(val)\n",
    "    numGrps = len(uniqueGrp)\n",
    "    # get modularity \n",
    "    mod = community.modularity(partition, G)\n",
    "    return partition, G, mod, numGrps\n",
    "\n",
    "### execution ###\n",
    "\n",
    "## run experiments\n",
    "def experiment(allDFs, szn, n_list, pid2pos_bref, pid2name, tempStr, destStr, valcol, returnPartition=False):\n",
    "    numAppear, idx_DFs, scaled_DFs = scale_and_count_DFs(allDFs, szn)\n",
    "    pcaDFs = pca_tf(allDFs, scaled_DFs)\n",
    "    allResults = clusterResults(idx_DFs, pcaDFs, n_list, pid2pos_bref)\n",
    "    countDict, countDictMatch = createCountDict(allResults, pid2name)\n",
    "    dict2DF(countDictMatch, pidszn2name, tempStr)\n",
    "    valDF = calcVals(countDict, numAppear, destStr)\n",
    "    nodeDF = valDF[['player1', 'player2']]\n",
    "    partition, G, mod, numGrp = getModularity(nodeDF, valDF[valCol])\n",
    "    \n",
    "    if returnPartition:\n",
    "        return partition, G, mod, numGrp\n",
    "    else:\n",
    "        return mod, numGrp\n",
    "    \n",
    "## USER TODO!!! Run experiments and save experDF to csv (can change to function to create experDF)\n",
    "Yr = ['19_20'] ## change as wanted\n",
    "seasonz = [['2019-20']] ## change as wanted\n",
    "valCols = ['val_3','val_6', 'raw'] ## change as wanted\n",
    "numdf = len(allDFs)\n",
    "n_lists = [[i]*numdf for i in range(2,151)] ## change as wanted\n",
    "## user define:\n",
    "name_of_exper = 'tbd'\n",
    "where2save = 'tbd'\n",
    "with open(f'{name_of_exper}.csv', 'w') as f:\n",
    "    f.write('szn,k,col,mod,numPartitions\\n')\n",
    "    for ix,szn in enumerate(seasonz):\n",
    "        for i,n_list in enumerate(n_lists):\n",
    "            k = n_list[0]\n",
    "            for col in valCols:\n",
    "                tempStr = '{}/{}_{}_{}.csv'.format(where2save,k,col,Yr[ix])\n",
    "                dictStr = '{}/dict{}_{}_{}.txt'.format(where2save,k,col,Yr[ix])\n",
    "                mod, numGrp = experiment(allDFs, szn, n_list, pid2pos_bref, pid2name, dictStr, tempStr, col, var2keep=0.99, returnPartition=False)#var2keep=0.99,\n",
    "                f.write('{},{},{},{},{}\\n'.format(Yr[ix],k,col,mod,numGrp))\n",
    "f.close()\n",
    "    \n",
    "## modularity frontier\n",
    "# helper\n",
    "def makeAndWriteGroupDF(partition, pid2pos, pid2name, year, show=False, saveStr=None):\n",
    "    \"\"\"\n",
    "    OUTPUT\n",
    "    partitionDF: translates partition (from community) into DF (pandas)\n",
    "    \"\"\"\n",
    "    partitionDF = pd.DataFrame.from_dict(partition, orient='index').reset_index()\n",
    "    partitionDF.columns=['id', 'group']\n",
    "    partitionDF['pos'] = 'nan'\n",
    "    partitionDF['name'] = 'nan'\n",
    "    partitionDF['season'] = 'nan'\n",
    "    for idx, row in partitionDF.iterrows():\n",
    "        if row.id in pid2pos_bref:\n",
    "            partitionDF.loc[idx,'pos'] = pid2pos[row.id]\n",
    "\n",
    "        player = row.id.split('_')\n",
    "        if int(player[0]) in pid2name:\n",
    "            partitionDF.loc[idx,'name'] = pid2name[int(player[0])]\n",
    "            partitionDF.loc[idx,'season'] = player[1]\n",
    "                    \n",
    "    # show\n",
    "    if show:\n",
    "        for cl in partitionDF['group'].unique():\n",
    "            temp = partitionDF[(partitionDF.group==cl)]\n",
    "            print('----------------------------------')\n",
    "            print('Group {} ({}) ({} players)'.format(cl, year, temp.shape[0]))\n",
    "            print('----------------------------------')\n",
    "            for idx, row in temp.iterrows():\n",
    "                print(row['name'], row.pos)\n",
    "    # write\n",
    "    if saveStr:\n",
    "        with open(saveStr, 'w') as f:\n",
    "            for cl in partitionDF['group'].unique():\n",
    "                temp = partitionDF[(partitionDF.group==cl)]\n",
    "                f.write('------------------------\\n')\n",
    "                f.write('Group {} ({} players)\\n'.format(cl, temp.shape[0]))\n",
    "                f.write('------------------------\\n')\n",
    "                for idx, row in temp.iterrows():\n",
    "                    f.write(row['name'] + ' ' + row['season'] + ',' + row.pos + '\\n')\n",
    "        f.close()\n",
    "\n",
    "    return partitionDF\n",
    "\n",
    "# calculate percentages for double mod frontier (with non single percent)\n",
    "def getNonSingles_Mod(year):\n",
    "    storeGrps = []\n",
    "    storeSingle = []\n",
    "    storeMod = []\n",
    "    storeNonSingle = []\n",
    "    for best_k in [int(i) for i in (np.arange(.2*400)[::2]+3)]:\n",
    "        ## read best exper\n",
    "        bestStr = '{directory_where_results_saved}{}_{}_{}.csv'.format(best_k,'val_6',year)\n",
    "        best = pd.read_csv(bestStr)\n",
    "\n",
    "        partition, G, mod, numGrps = getModularity(best[['player1', 'player2']], best['val_6'])\n",
    "        storeGrps.append(numGrps)\n",
    "        storeMod.append(mod*100)\n",
    "\n",
    "        saveStr_ = '{}/groups_{}.txt'.format(directory_saved,year)\n",
    "        partitionDF = makeAndWriteGroupDF(partition,pid2pos_bref,pid2name,year=year, show=False, saveStr=False)\n",
    "        numSingle = 0\n",
    "        for cl in partitionDF['group'].unique():\n",
    "            shape = partitionDF[(partitionDF['group']==cl)].shape[0]\n",
    "            if shape == 1:\n",
    "                numSingle += 1\n",
    "        nonSingle = ((numGrps-numSingle)/numGrps)*100\n",
    "        storeSingle.append(numSingle)\n",
    "        storeNonSingle.append(nonSingle)\n",
    "    return storeSingle, storeNonSingle, storeMod, storeGrps\n",
    "\n",
    "def modFrontier3(experDF, hix, year, storeSingle, storeNonSingle, storeMod, storeGrps):\n",
    "    \"\"\"\n",
    "    INPUT \n",
    "    experDF: output of running many experiments; cols = [szn,k,col,mod,numPartitions]; read in csv created or create function to store experDF\n",
    "    hix: specified upper bound for graphing modularity frontier\n",
    "    year: chosen season\n",
    "    \n",
    "    OUTPUT\n",
    "    graph (plotly): last step of CDkM; user must analyze and choose best k\n",
    "    \"\"\"\n",
    "    cond = hix\n",
    "    exper_ = experDF[(experDF['szn']==year)]\n",
    "    df = exper_[(exper_['col']=='val_6')]\n",
    "    fig = go.Figure()\n",
    "    temp = df[(df.numPartitions<=hix)]\n",
    "\n",
    "    modPercent = [100*i for i in temp['mod']]\n",
    "    fig.add_trace(go.Scatter(x=temp['numPartitions'], y=storeMod,#modPercent\n",
    "                             marker=dict(\n",
    "        size=5,\n",
    "        color=temp['k'], #set color equal to a variable\n",
    "        colorscale='Portland', # one of plotly colorscales\n",
    "        showscale=True, colorbar=dict(\n",
    "            title=\"k-values\")\n",
    "    ) ,hovertext=[str(i) for i in [int(i) for i in (np.arange(.2*400)[::2]+3)]],#list(zip(temp['col'], temp['k']))],\n",
    "    hoverinfo=\"text\",\n",
    "               mode='markers',showlegend=False))\n",
    "\n",
    "\n",
    "    def condition(x): return x <= cond\n",
    "    boolArr = condition(np.array(storeGrps))\n",
    "    x = list(compress(storeGrps, boolArr))\n",
    "    y1=list(compress(storeMod, boolArr))\n",
    "\n",
    "\n",
    "    def log_func(x,a,b):\n",
    "        return a+b*np.log(x)\n",
    "\n",
    "    popt1, pcov = curve_fit(lambda t,a,b: a+b*np.log(t),  x,  y1)\n",
    "\n",
    "\n",
    "    xx = np.linspace(3, cond, 1000)\n",
    "    yy1 = log_func(xx, *popt1)\n",
    "    y2 = list(compress(storeNonSingle, boolArr))\n",
    "\n",
    "    def exp_func(x, a, b,c,d):\n",
    "        return a * np.exp(-b * x+c)+d\n",
    "\n",
    "    popt2, pcov = curve_fit(lambda t,a,b,c,d: a * np.exp(-b * t+c)+d, x, y2)#curve_fit(exp_func, x, y2)\n",
    "    yy2 = exp_func(xx, *popt2)\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=xx, y=yy1, showlegend=False\n",
    "    ))\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # non singletons\n",
    "    fig.add_trace(go.Scatter(x=x, y=storeNonSingle, mode='markers', name='Non-Singleton (NS) Groups - Experiment (%)', \n",
    "        marker=dict(\n",
    "        symbol='13',\n",
    "        size=7,\n",
    "        color=temp['k'], #set color equal to a variable\n",
    "        colorscale='Portland', # one of plotly colorscales\n",
    "        showscale=True, colorbar=dict(\n",
    "            title=\"k-values\")\n",
    "    ), showlegend=True,hovertext=[str(i) for i in list(temp['k'])]))#, name='nonSingleVals'))\n",
    "\n",
    "    # modularity\n",
    "    fig.add_trace(go.Scatter(x=x, y=storeMod, mode='markers', name='Modularity (Mod) - Experiment (%)',\n",
    "        marker=dict(\n",
    "        symbol='19',\n",
    "        size=7,\n",
    "        color=temp['k'], #set color equal to a variable\n",
    "        colorscale='Portland', # one of plotly colorscales\n",
    "        showscale=True, colorbar=dict(\n",
    "            title=\"k-values\")\n",
    "    ), showlegend=True,hovertext=[str(i) for i in list(temp['k'])]))#, name='modVals'))\n",
    "\n",
    "\n",
    "    # approximations\n",
    "    fig.add_trace(go.Scatter(x=xx, y=yy2, mode='markers', showlegend=True, name='NS Groups - Approximated (%)',marker=dict(size=2)))\n",
    "    fig.add_trace(go.Scatter(x=xx, y=yy1, mode='markers', showlegend=True, name='Mod - Approximated (%)',marker=dict(size=2)))\n",
    "    # fig.update_traces(marker_size=4)\n",
    "    fig.update_layout(legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ))\n",
    "\n",
    "\n",
    "    # line\n",
    "    fig.update_layout(\n",
    "        shapes=[\n",
    "            dict(type=\"line\", xref=\"x\", yref=\"y\",\n",
    "                x0=5, y0=0.3, x1=hix, y1=0.3, line_width=3)]\n",
    "    )\n",
    "\n",
    "    # arrows\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            mode='markers',\n",
    "            x=[5,hix],\n",
    "            y=[0.3,0.3],\n",
    "            marker_symbol=[7,8],\n",
    "            showlegend=False,marker_line_width=2, marker_size=12,marker_line_color=\"midnightblue\", marker_color=\"midnightblue\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # fig.show()\n",
    "\n",
    "    # bias var\n",
    "    fig.add_annotation(x=7, y=0.32,\n",
    "                text='Bias',\n",
    "                showarrow=False,\n",
    "                yshift=10)\n",
    "\n",
    "    fig.add_annotation(x=hix-2, y=0.32,\n",
    "                text='Variance',\n",
    "                showarrow=False,\n",
    "                yshift=10)\n",
    "\n",
    "    \n",
    "    font_ = \"Times New Roman\"\n",
    "    fig.update_layout(\n",
    "    font_family=font_,\n",
    "    title_font_family=font_,\n",
    ")\n",
    "            \n",
    "\n",
    "    fig.update_layout(title=f'{szn} Modularity Frontier', xaxis_title='Number of Macro Clusters', yaxis_title='Percent', title_x=0.5)\n",
    "\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
